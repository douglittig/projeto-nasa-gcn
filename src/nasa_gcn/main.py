"""
NASA GCN Pipeline - Entry Point

Este m√≥dulo √© o ponto de entrada para execu√ß√£o via Databricks Jobs.
Executa valida√ß√µes e exibe estat√≠sticas do pipeline, incluindo m√©tricas
de linhas processadas na √∫ltima execu√ß√£o do DLT.
"""

from databricks.sdk.runtime import spark


# Configura√ß√µes do pipeline
CATALOG = "sandbox"
SCHEMA = "nasa_gcn_dev"

# Mapeamento de tabelas por camada (Medallion Architecture)
TABLE_LAYERS = {
    "ü•â BRONZE": ["gcn_raw"],
    "ü•à SILVER": [
        "gcn_classic_text",
        "gcn_classic_voevent", 
        "gcn_classic_binary",
        "gcn_notices",
        "gcn_circulars",
        "igwn_gwalert",
        "gcn_heartbeat",
    ],
    "ü•á GOLD": ["gcn_events_summarized"],
}


def get_pipeline_id():
    """
    Obt√©m o Pipeline ID do DLT dinamicamente.
    Procura por pipelines que escrevem no schema configurado.
    """
    from databricks.sdk import WorkspaceClient
    
    try:
        w = WorkspaceClient()
        pipelines = list(w.pipelines.list_pipelines())
        
        for pipeline in pipelines:
            # Procura pelo pipeline que usa nosso schema (considera prefixo [dev ...])
            # Ex: "[dev dltreinamentos_data] nasa_gcn_pipeline" ou "nasa_gcn_pipeline"
            if pipeline.name and "nasa_gcn" in pipeline.name.lower():
                return pipeline.pipeline_id
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao obter Pipeline ID: {e}")
        return None


def get_dlt_metrics(pipeline_id: str) -> dict:
    """
    Consulta o event_log do DLT para obter m√©tricas da √∫ltima execu√ß√£o.
    
    Retorna um dicion√°rio com o n√∫mero de linhas processadas por tabela:
    {"table_name": num_output_rows, ...}
    
    Nota: Tabelas streaming (Bronze/Silver) podem n√£o reportar num_output_rows
    da mesma forma que tabelas batch (Gold).
    """
    if not pipeline_id:
        return {}
    
    try:
        # Query para obter m√©tricas de flow_progress da √∫ltima execu√ß√£o
        # O event_log() √© uma table-valued function do Unity Catalog
        # 
        # Status poss√≠veis: QUEUED, STARTING, RUNNING, COMPLETED, FAILED, SKIPPED, STOPPED, IDLE, EXCLUDED
        # - Tabelas batch (dlt.read) geralmente reportam COMPLETED
        # - Tabelas streaming (dlt.read_stream) podem reportar IDLE ou RUNNING
        query = f"""
        WITH latest_update AS (
            -- Encontra o update_id mais recente
            SELECT origin.update_id
            FROM event_log('{pipeline_id}')
            WHERE event_type = 'create_update'
            ORDER BY timestamp DESC
            LIMIT 1
        ),
        flow_metrics AS (
            -- Extrai m√©tricas de cada flow (tabela) do √∫ltimo update
            -- N√£o filtra por status espec√≠fico para capturar streaming e batch
            SELECT 
                origin.flow_name AS table_name,
                details:flow_progress:status::STRING AS flow_status,
                details:flow_progress:metrics:num_output_rows::LONG AS rows_processed
            FROM event_log('{pipeline_id}')
            WHERE event_type = 'flow_progress'
              AND origin.update_id = (SELECT update_id FROM latest_update)
              AND details:flow_progress:metrics:num_output_rows IS NOT NULL
        )
        SELECT table_name, flow_status, SUM(rows_processed) as rows_processed
        FROM flow_metrics
        GROUP BY table_name, flow_status
        """
        
        result = spark.sql(query).collect()
        
        # Normaliza nomes: flow_name vem como "catalog.schema.table", queremos s√≥ "table"
        metrics = {}
        for row in result:
            full_name = row.table_name
            # Extrai apenas o nome base da tabela (√∫ltima parte ap√≥s o √∫ltimo ponto)
            table_basename = full_name.split(".")[-1] if full_name else full_name
            # Soma caso haja m√∫ltiplos registros para a mesma tabela
            if table_basename in metrics:
                metrics[table_basename] += row.rows_processed
            else:
                metrics[table_basename] = row.rows_processed
        
        return metrics
    
    except Exception as e:
        print(f"‚ö†Ô∏è  N√£o foi poss√≠vel obter m√©tricas DLT: {e}")
        return {}


def get_pipeline_stats():
    """Retorna estat√≠sticas das tabelas do pipeline GCN (contagem total)."""
    stats = {}
    
    for layer_name, tables in TABLE_LAYERS.items():
        stats[layer_name] = {}
        for table_name in tables:
            full_name = f"{CATALOG}.{SCHEMA}.{table_name}"
            try:
                count = spark.table(full_name).count()
                stats[layer_name][table_name] = count
            except Exception as e:
                stats[layer_name][table_name] = f"Error: {e}"
    
    return stats


def format_number(value) -> str:
    """Formata n√∫mero com separador de milhar ou retorna string de erro."""
    if isinstance(value, int):
        return f"{value:,}"
    return str(value)


def main():
    """Fun√ß√£o principal executada pelo Databricks Job."""
    print("=" * 60)
    print("NASA GCN Pipeline - Status Report")
    print("=" * 60)
    
    # Obt√©m contagens totais das tabelas
    stats = get_pipeline_stats()
    
    # Obt√©m m√©tricas DLT da √∫ltima execu√ß√£o
    pipeline_id = get_pipeline_id()
    dlt_metrics = get_dlt_metrics(pipeline_id) if pipeline_id else {}
    
    if dlt_metrics:
        print(f"\nüìä M√©tricas da √∫ltima execu√ß√£o do pipeline")
        print("-" * 40)
    
    for layer, tables in stats.items():
        print(f"\n{layer}")
        print("-" * 40)
        
        for table_name, total_count in tables.items():
            total_str = format_number(total_count)
            
            # Verifica se temos m√©tricas DLT para esta tabela
            rows_processed = dlt_metrics.get(table_name)
            
            if rows_processed is not None and rows_processed > 0:
                print(f"  ‚Ä¢ {table_name}: {total_str} (total) | +{rows_processed:,} (√∫ltima execu√ß√£o)")
            else:
                print(f"  ‚Ä¢ {table_name}: {total_str}")
    
    print("\n" + "=" * 60)
    print("Pipeline executado com sucesso!")
    print("=" * 60)


if __name__ == "__main__":
    main()
